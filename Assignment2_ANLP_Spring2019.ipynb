{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2, due March 27, 10amÂ¶\n",
    "\n",
    "### Late submission policy: each late day is 10% of the grade\n",
    "In this assignment you will classify the Movie Reviews Corpus into positive or negative. \n",
    "This is the polarity data set (1000 negative and 1000 positive reviews). For more information visit Bo Pang and Lillian Lee's Movie Review Site: http://www.cs.cornell.edu/people/pabo/movie-review-data/.\n",
    "This corpus is part of the NLTK distribution. \n",
    "\n",
    "\n",
    "### Packages\n",
    "First import all the packages that you will need during this assignment.\n",
    "\n",
    "numpy (www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "NLTK (https://www.nltk.org/) is the NLTK tool.\n",
    "pandas (https://pandas.pydata.org/) is the fundamental package for for data manipulation and analysis (we will use dataframes).\n",
    "sklearn (http://scikit-learn.org/stable/) provides simple and efficient tools for data mining and data analysis.\n",
    "matplotlib (http://matplotlib.org) is a library for plotting graphs in Python.\n",
    "keras (https://keras.io/) is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, LSTM, Flatten, Activation\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews,stopwords\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import wordnet\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), \" \".join((list(movie_reviews.words(fileid)))), category)\n",
    "    for category in movie_reviews.categories()\n",
    "    for fileid in movie_reviews.fileids(category)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe called imdb where the data will be stored. \n",
    "# The imdb dataframe should have three columns: words (tokenized text), text, label (pos or neg)\n",
    "imdb = None\n",
    "\n",
    "imdb = pd.DataFrame(documents, columns=['words', 'text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert the pos/neg labels into binary labels: 0 - positive; 1 - negative;\n",
    "imdb['target'] = pd.Categorical(imdb.label, categories = ['pos','neg']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your output should look like: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                                                words\t                                        text\tlabel\ttarget\n",
    "0\t[plot, :, two, teen, couples, go, to, a, churc...\tplot : two teen couples go to a church party ,...\tneg\t1\n",
    "1\t[the, happy, bastard, ', s, quick, movie, revi...\tthe happy bastard ' s quick movie review damn ...\tneg\t1\n",
    "2\t[it, is, movies, like, these, that, make, a, j...\tit is movies like these that make a jaded movi...\tneg\t1\n",
    "3\t[\", quest, for, camelot, \", is, warner, bros, ...\t\" quest for camelot \" is warner bros . ' first...\tneg\t1\n",
    "4\t[synopsis, :, a, mentally, unstable, man, unde...\tsynopsis : a mentally unstable man undergoing ...\tneg\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb.tail() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your output should look like: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "                                                     words\t                                        text\tlabel\ttarget\n",
    "1995\t[wow, !, what, a, movie, ., it, ', s, everythi...\twow ! what a movie . it ' s everything a movie...\tpos\t0\n",
    "1996\t[richard, gere, can, be, a, commanding, actor,...\trichard gere can be a commanding actor , but h...\tpos\t0\n",
    "1997\t[glory, --, starring, matthew, broderick, ,, d...\tglory -- starring matthew broderick , denzel w...\tpos\t0\n",
    "1998\t[steven, spielberg, ', s, second, epic, film, ...\tsteven spielberg ' s second epic film on world...\tpos\t0\n",
    "1999\t[truman, (, \", true, -, man, \", ), burbank, is...\ttruman ( \" true - man \" ) burbank is the perfe...\tpos\t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data points to randomize the order\n",
    "\n",
    "imdb = shuffle(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new data column with the number of tokens per text \n",
    "# We will use this information to specify the maximum length of the sentences that we will analyze\n",
    "\n",
    "imdb['num_words'] = imdb.words.apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the disctribution of the text length across data points. \n",
    "# We will analyze up to 1000 tokens per document (data point)\n",
    "\n",
    "imdb['bins']=pd.cut(imdb.num_words, bins=[0,200,500,900,1400, np.inf], labels=['0-200', '200-500', '500-900','900-1400' ,'>1400'])\n",
    "word_distribution = imdb.groupby('bins').size().reset_index().rename(columns={0:'counts'})\n",
    "word_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(imdb) * .7) \n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(imdb) - train_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate document representations from labels\n",
    "\n",
    "train_docs = imdb['text'][:train_size]\n",
    "train_labels = imdb['target'][:train_size]\n",
    "\n",
    "test_docs = imdb['text'][train_size:]\n",
    "test_labels = imdb['target'][train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create model 1.\n",
    "\n",
    "## Use the model that was created for Assignment 1. \n",
    "## Report the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Tokenizer class (https://keras.io/preprocessing/text/) allows to vectorize a text corpus.\n",
    "# Follow the provided link to learn about all the arguments for Tokenizer\n",
    "\n",
    "max_words = 1000   # as discussed above, this is the max number of tokens alanyzed per text\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report results here\n",
    "print (None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Do  Download the glove.6B.100d.txt file (100 dimentions)\n",
    "\n",
    "### the glove.6B.100d.txt file should be in the same directory (root directory) as your notebook file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create model 2.\n",
    "\n",
    "## Create a model using word embeddings. \n",
    "## Compare the two models.\n",
    "## Report the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out of 400000 word vectors, use only those that correspond to the words used in the movie review corpus. \n",
    "# vocab_size contains the number of different words used in the movie review corpus\n",
    "\n",
    "vocab_size = len(tokenize.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The vocabulary size should be: 34211"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in tokenize.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "# in this model, in contrast to Assignment 1, instead of useing hidden units, we use GloVe word embeddings\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_words, trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "# compile the model\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model here\n",
    "# Use your code from assignment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model here\n",
    "# Use your code from assignment1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (None) # how the two models are different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Experiment with another set of word embeddings.\n",
    "\n",
    "## Run the same model using a different embeddings matrix (for example, different size for GloVe; word2Vec; embeddings obtained from different corpora). Compare the obtained reslut and the result from part 2. \n",
    "\n",
    "### Compare the two models.\n",
    "### Report the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (None) # how the two embeddings models are different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. (optional) Experiment with word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the GloVe matrix so that it contains all  400000 word vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can find many online resources with word vector operations. For example, \n",
    "### https://datascience-enthusiast.com/DL/Operations_on_word_vectors.html\n",
    "\n",
    "### Use these functions, or write your own functions to find intersting inforamtion / connections that could be deduced using the word embedings. \n",
    "\n",
    "### Report your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
