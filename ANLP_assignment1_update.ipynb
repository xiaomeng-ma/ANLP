{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, due March 6, 10am\n",
    "\n",
    "### Late submission policy: each late day is 10% of the grade\n",
    "\n",
    "In this assignment you will classify the a collection of BBC documents. These documents are divided into five topics: 'business', 'entertainment', 'politics', 'sport', 'tech'.\n",
    "\n",
    "The document collection can be downloaded following the link below. Store the downloaded file in the same directory where you will create your assignlemt solution file. \n",
    "\n",
    "https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\n",
    "\n",
    "### 1. Packages\n",
    "\n",
    "First import all the packages that you will need during this assignment.\n",
    "- numpy (www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- pandas (https://pandas.pydata.org/) is the fundamental package for  for data manipulation and analysis (we will use dataframes).\n",
    "- sklearn (http://scikit-learn.org/stable/) provides simple and efficient tools for data mining and data analysis.\n",
    "- matplotlib (http://matplotlib.org) is a library for plotting graphs in Python.\n",
    "- keras (https://keras.io/) is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get the dataset\n",
    "\n",
    "The data set consists of 2225 data points. Each data point is a BBC document (all small letters, no puctuation marks, white spaces in-bewteen words) labeled with one of the five topics.\n",
    "\n",
    "Let's get the data set and study it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNMENT 1: make sure that your data is stored together with the Python notebook\n",
    "\n",
    "data  = pd.read_csv('bbc-text.csv')\n",
    "# insert a number in the parenthesis to see more (less) data points; or try to remove the .head command alltogether; \n",
    "data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the label disctribution in the dataset\n",
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the visualization at the label disctribution in the dataset\n",
    "my_tags = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "plt.figure(figsize=(10,4))\n",
    "data.label.value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split the dataset into training and testing (70 / 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNMENT 2: insert the correct value instead of None to obtain the train and test data sets of the expected size\n",
    "\n",
    "train_size = int(len(data) * None) \n",
    "print (\"Train size: %d\" % train_size)\n",
    "print (\"Test size: %d\" % (len(data) - train_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output: \n",
    "\n",
    "        Train size: 1557\n",
    "        Test size: 668"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate document representations from labels\n",
    "\n",
    "train_docs = data['text'][:train_size]\n",
    "train_labels = data['label'][:train_size]\n",
    "\n",
    "test_docs = data['text'][train_size:]\n",
    "test_labels = data['label'][train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this assignment we will use a simple bag of words representaiton of documents. \n",
    "\n",
    "We will use one hot representation (the word is either in or out) for the 1000 words most frequently used in this corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Tokenizer class (https://keras.io/preprocessing/text/) allows to vectorize a text corpus.\n",
    "# Follow the provided link to learn about all the arguments for Tokenizer\n",
    "\n",
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling fit_on_texts() automatically creates a word index lookup of the corpus vocabulary.\n",
    "\n",
    "# Using texts_to_matrix method we create the training data that weâ€™ll pass our model; \n",
    "# and test data on which the model will be testd.\n",
    "\n",
    "\n",
    "### ERROR WAS HERE: incorrect name of the variable\n",
    "tokenize.fit_on_texts(train_docs) # only fit on train\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_docs)\n",
    "x_test = tokenize.texts_to_matrix(test_docs)\n",
    "\n",
    "# ASSIGNMENT 3: briefly write (within the print() statement below) why fit_on_texts has only on train_docs as its parameter\n",
    "\n",
    "print (None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn utility to convert label strings to numbered index\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "### ERROR WAS HERE: incorrect name of the variable\n",
    "encoder.fit(train_labels)\n",
    "\n",
    "y_train = encoder.transform(train_labels)\n",
    "y_test = encoder.transform(test_labels)\n",
    "\n",
    "# Check the resulting labels and the shape of the labels\n",
    "print(\"y_train\\n\",y_train)\n",
    "print(\"y_test\\n\",y_test)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "\n",
    "# ASSIGNMENT 4: briefly explain (within the print() statement below) \n",
    "# why the initial label 1-dim vector was converted into 2-dim vector\n",
    "\n",
    "print (None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build a model \n",
    "\n",
    "### We will use a two-layer sequential model. \n",
    "### Number of hidden units: \n",
    "use 100 hiddent units to be used for the submission. For better understanding of how NN works try to change the number of hidden units, some values to try: 2, 32, max_words.\n",
    "### Dropout rate: \n",
    "use 0.5 dropout rate for the submission. For better understanding of how NN works try to change the dropout rate, some values to try: 0.1, 0.9, comment out the Dropout part of the model.\n",
    "### Activation functions: \n",
    "for the submission, use tanh activation for the hidden units and sigmoid for the output units. For better understanding of how NN works try to change the activation finctions on both hidden and output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of epoches to be used for the assignment submission is 3, \n",
    "# however you can try to update this parameter and see what happnens. \n",
    "\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ERROR WAS HERE: missing cell\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "\n",
    "number_hidden_units = 100 \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(number_hidden_units, input_shape=(max_words,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))   # this is the output layer\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# ASSIGNMENT 5: output (within the print() statement below) \n",
    "# how many output units are there in the created model. \n",
    "\n",
    "print (\"Number of output units = \", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model above. \n",
    "# model.fit trains the model\n",
    "\n",
    "# The validation_split param tells Keras what % of our training data should be used in the validation set\n",
    "\n",
    "# See the prograssion of the accuracy and validation loss.\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model above. \n",
    "# ASSIGNMENT 6: test your model using the commands below\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output: \n",
    "\n",
    "        Test accuracy: 0.947"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The below cells contain functions and and commands for the visualization of the obtained results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This utility function is from the sklearn \n",
    "# docs: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=30)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, fontsize=22)\n",
    "    plt.yticks(tick_marks, classes, fontsize=22)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label', fontsize=25)\n",
    "    plt.xlabel('Predicted label', fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = model.predict(x_test)\n",
    "\n",
    "y_test_1d = []\n",
    "y_pred_1d = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    probs = y_test[i]\n",
    "    index_arr = np.nonzero(probs)\n",
    "    one_hot_index = index_arr[0].item(0)\n",
    "    y_test_1d.append(one_hot_index)\n",
    "\n",
    "### ERROR WAS HERE: incorrect name of the variable\n",
    "for i in range(0, len(y_predicted)):\n",
    "\n",
    "    probs = y_predicted[i]\n",
    "    predicted_index = np.argmax(probs)\n",
    "    y_pred_1d.append(predicted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how to generate a prediction on individual examples\n",
    "text_labels = encoder.classes_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test_1d, y_pred_1d)\n",
    "plt.figure(figsize=(24,20))\n",
    "plot_confusion_matrix(cnf_matrix, classes=text_labels, title=\"Confusion matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
